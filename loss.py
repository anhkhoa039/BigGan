import torch


# Wasserstein loss for discriminator
def wasserstein_loss_discriminator(real_score, fake_score):
    return -torch.mean(real_score) + torch.mean(fake_score)

# Wasserstein loss for generator
def wasserstein_loss_generator(fake_score):
    return -torch.mean(fake_score)

def gradient_penalty(netD, real_data, fake_data, aux_labels):
    """
    Compute the gradient penalty for the WGAN-GP loss.
    ===
    :param netD: The discriminator network.
    :param real_data: Real images from the dataset.
    :param fake_data: Fake images generated by the generator.
    :param aux_labels: Labels corresponding to the data.
    :return: Gradient penalty scalar.
    """
    batch_size = real_data.size(0)
    alpha = torch.rand(batch_size, 1, 1, 1, device=real_data.device)
    interpolates = alpha * real_data + (1 - alpha) * fake_data
    interpolates.requires_grad_(True)

    dis_output = netD(interpolates, aux_labels)

    gradients = torch.autograd.grad(
        outputs=dis_output,
        inputs=interpolates,
        grad_outputs=torch.ones_like(dis_output),
        create_graph=True,
        retain_graph=True,
        only_inputs=True,
    )[0]

    gradients = gradients.view(gradients.size(0), -1)  # Flatten
    gradient_norm = gradients.norm(2, dim=1)  # L2 norm
    penalty = ((gradient_norm - 1) ** 2).mean()  # Gradient penalty
    return penalty
